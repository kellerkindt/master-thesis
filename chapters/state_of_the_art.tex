\chapter{State of the art}

In this chapter, programs solving similar problems - as described before - are looked further into.
Some might manifest as middle-ware, some might teach about proven strategies and some might show pitfalls one should avoid.

\subsection{Hadoop MapReduce}
\label{hadoop}
\label{mapreduce}

focus transforming a big dataset by splitting it into many jobs, distributing it onto many workers, doing a transformation on each dataset, and merging it back together (only map -> reduce)
Distributed filesystem

For big data transformation, Hadoop MaReduce is well known \todo{prove}.
With MapReduce, the input data is split into blocks of data and distributed onto Mappers.
Mappers then apply the business logic and output intermediate results in form of key/value pairs.
After shuffling, the Reduce stage will combine values from key/value pairs with the same key.
In the final output, each key is unique.

This strategy very powerful to process large input data.
Mappers and Reducers can work independently on their data-sets and therefore scale very well when adding further instances \todo{prove}.

Using MapReduce in the computer vision pipeline could be implemented like the following:
\begin{itemize}
	\item Each video is split into many frames and each frame is applied to a mapper
	\item A mapper tries to detect all vehicles on a frame and outputs the position of it
	\item The Reduce then tries to link the detections of a vehicle for each frame
	\item The final result would be a set of detections and therefore positions of each vehicle in the video
\end{itemize}

But at the moment, this approach seems to be unfitting due to at least two facts:

\begin{enumerate}
	\item It is not always trivial to reasonable link the detections of a vehicle.
	For example, a vehicle can be hidden behind a tree for a few frames until visible again.
	In addition, MapReduce requires the combination to be performed per common key.
	Until one is trying to link the detections of multiple frames, the is no common identifier that could easily be used as key.
	The position of a moving vehicle cannot be used as key, neither can the color or size, because of the noise of the camera, deviation in detection output and perspective distortions.
	\item MapReduce is great in combining many machines to solve a big computational problem.
	But at the moment, this is neither a desired nor given condition. At the moment, there a \todo{three} very powerful workstations with special hardware (GPUs).
	Therefore it is perfectly acceptable, when each workstation works through a complete video at a given time instead.
\end{enumerate}

\subsection{Build Pipelines}

Build pipelines such as GitLab\cite{gitlab:main} and Jenkins\cite{jenkins:main} provide multi stage work distribution on multiple machines.
In a common use-case, this is used to build binaries out of source code, after a new commit into the \todo{SCM - Source Code Management} repository was made.
At IT-Designers GmbH both are being used for scenarios like this.
A pipeline definition in GitLab CI/CD \cite{gitlab:ci:yaml} or a Jenkinsfile \cite{jenkins:pipeline:jenkinsfile} describe stages to be executed.
Each stage can be hosted on another node and be executed sequential or in parallel.
Although this seems to be quite fitting for the desired workflow, there are two issues.
First of all, such a pipeline does not involve any user input besides an optional manual start command.
The result is then determined based on the input repository.
Second, such an pipeline is designed to determine the output (usually compile) whereas each run ins independent from the previous.
Usually a new run is caused by an change of the input data.
However, the desired workflow differs in this aspect.
A redo of a stage can depend on the result of the previous run (if the results are poor).
For a project here, the input (video file), does not change for each run, but is tightly coupled with all stages of the pipeline.
Intermediate results need to influence further stages, even if repeated.

\subsection{Docker Integration}



\section{Existing software solutions}

IBM InfoSpheere
\cite{infosphere:datastage}

GitLab \cite{gitlab:ci:yaml}

Jenkins \cite{jenkins:pipeline:jenkinsfile}

Quartz?? \cite{quartz:quickstart}

CSCS High Throughput Scheduler?? \cite{cscs:high_throughput}


qsub job submission % bad \url{https://wiki.uiowa.edu/display/hpcdocs/Basic+Job+Submission}




\subsection{Quartz}

\cite{quartz:main}
\cite{quartz:overview}

\begin{itemize}
	\item + Java
	\item - requires integration
	\item - aimed towards running a job at a given time or in certain intervals
\end{itemize}

\subsection{Pipeline examples: Jenkins / GitLab}

Pipeline file with multiple stages
a stage can be executed on a host
focused on doing a job with different inputs again and again and again
CI -> usually no user interaction

\subsection{Camunda}

\cite{camunda:main}
\cite{camunda:process_engine_api}
\cite{camunda:rest_api_reference}

Rich Business Process Management tool, many types of tasks, steps, transitions, triggers and endpoints.
Focused upon moving a dataset along the matching path of the process.
Out of the box graphical user interface for process definition and interaction.
Allows custom external worker through queues.
Misses capability to control which task to process on which worker through fine grained filters and how to allocate and distribute resources(?).
Requires custom plugins for more advanced user forms, not designed for that.
Not designed provide an overview on the docker machines, cluster state nor logs, file up and download

\subsection{Cubernetes}

too heavy?

\subsection{Luigi}

Similar, but locked-down on python  (+machine learning)?
\cite{luigi:etc:distributed_pipelines}

\subsection{Celery}

\cite{celery:main}

\subsection{Nomad}

\todo{that dude that did the webui}

\cite{nomad:main}
Deployment and management of containers
rich REST API
can handle resource requirements
device plugins / GPU support

vs kubernetes \cite{nomad:vs:kubernetes}

++ available through debian / ubuntu std-repositories

\section{Docker}
