\chapter{Literature Survey}
%\chapter{State of the art}
\label{state_of_the_art}

This chapter is about accumulating information.
Programs that are solving similar problems, as described in the desired workflow, or dealing with a subset of the problem are looked into.
Fundamental knowledge for understanding this thesis is also acquired here.
\section{Similar solutions}

This sections focuses on programs trying to provide somewhat similar workflows.
The reason for this is to use well established or suitable programs as middle-ware to reduce implementation overhead, or, where this is not possible, one might be able to gather ideas and learn about proven strategies to use or pitfalls to avoid while implementing custom solutions.


\subsection{Hadoop MapReduce}
\label{hadoop}
\label{hadoopfs}
\label{mapreduce}

For big data transformation, Hadoop MapReduce\cite{hdfs:main} is well known.
With MapReduce, the input data is split into blocks of data and distributed onto Mappers.
Mappers then apply the business logic and output intermediate results in form of key/value pairs.
After shuffling, the Reduce stage will combine values from key/value pairs with the same key.
In the final output, each key is unique accompanied with a value.

This strategy has proven to be very powerful to process large amount input data because Mappers and Reducers can work independently on their data-sets and therefore scale very well when adding further instances.

If the implementation were to be based on Hadoop MapReduce to achieve the desired workflow, it could be done like the following:
\begin{itemize}
	\item Each video is split into many frames and each frame is applied to a Mapper
	\item A Mapper tries to detect all vehicles on a frame and outputs their position, orientation, size and so on
	\item The Reducer then tries to link the detections of a vehicle through multiple frames
	\item The final result would be a set of detections and therefore all positions for each vehicle in the video
\end{itemize}

But at the moment, this approach seems to be unfitting due to at least the following reasons:

\begin{enumerate}
	\item It is not always trivial to reasonable link the detections of a vehicle.
	For example, a vehicle can be hidden behind a tree for a few frames until visible again.
	In addition, MapReduce requires the combination to be performed per common key.
	Until one is trying to link the detections of multiple frames, there is no common identifier that could easily be used as key.
	The position of a moving vehicle cannot be used as key, neither can the color or size, because of the noise of the camera, deviation in detection output and perspective distortions.
	The current implementation of the TrackerApplication is archiving this by finding similarities between detections, but for the Mapper it would be required to express this as a deterministic key.
	\item MapReduce is great in combining many machines to solve a big computational problem.
	But at the moment, this is neither a desired nor given condition. At the moment, there are a handful of very powerful workstations with specialized hardware.
	Therefore it is perfectly acceptable and sometimes required, when each workstation works through a complete video at a given time instead.
\end{enumerate}

\subsection{Build Pipelines}

Build pipelines such as GitLab\cite{gitlab:main} and Jenkins\cite{jenkins:main} can also distribute the execution of stages onto other server nodes.
In a common use-case, such build pipelines are used to build binaries out of source code, after a new commit into a SCM\footnote{Source Code Management} repository was made.
At IT-Designers GmbH GitLab as well as Jenkins are commonly used for scenarios exactly like this.
A pipeline definition in GitLab CI/CD \cite{gitlab:ci:yaml} or in a Jenkinsfile \cite{jenkins:pipeline:jenkinsfile} describe stages and commands to execute.
Each stage can be hosted on another node and be executed sequential or in parallel to each other.

Although this seems to be quite fitting for the desired workflow, there are two issues.
First of all, such a pipeline does not involve any user input besides an optional manual start invocation.
The result is then determined based on the state of the input repository.
Second, such a pipeline is designed to determine the output (usually by compiling) whereas each run is independent from the previous and a repeated run shall provide the same result as the previous did.
Usually, a new run is only caused by a change of the input data.
However, the desired workflow differs in this aspects.
A redo of a stage can depend on the result of the previous stage, for example, if the results are poor or the the stage failed.
Instead of having multiple complete pipeline runs per project, the desired workflow uses a pipeline definition as base for which the order can be changed.
Also, intermediate results need to influence further stages, even if repeated.


\subsection{Camunda}

Camunda\cite{camunda:main} calls itself a \enquote{Rich Business Process Management tool} and allows the user to easily create new pipelines by combining existing tasks with many triggers and custom transitions. % , many types of tasks, steps, transitions, triggers and endpoints.
Camunda is focused upon visualizing the flow and tracking the data through a pipeline. %moving a dataset along the matching path of the process.
The Camundas Process Engine\cite{camunda:process_engine_api} also allows user intervention between tasks.

One of the main supporting reason for it Camunda is the out of the box rich graphical user interface for process definition and interaction.
Through its API\cite{camunda:rest_api_reference}, Camunda also allows custom external workers to execute a task.
But it misses the capability to control which task shall be processed on which worker node which is required by the desired workflow.
It does also not provide any concept on how to allocate and distribute resources.
The user interface - while being rich overall - is quite rudimentary when it is about configuring tasks and would therefore require custom plugins to be developed for more advanced user interactions.

Camunda is also not designed to reorder stages or insert user interactions at seemingly random fashion.
The user itself is considered more as a worker that gets some request, \enquote{executes} this externally and finally marks the request as accepted or declined.
Mapping this to the desired workflow does not feel intuitive.
Finally, there is also no overview of task executors, no centralized log accumulation and no file up- or download for global project resources.


\subsection{Nomad}
\label{nomad}

Nomad\cite{nomad:main} by HashiCorp is a tool to deploy, manage and monitor containers, whereas each job is executed in its own container.
It provides a rich REST API and can consider hardware constraints on job submissions.
Compared to Kubernetes\cite{nomad:vs:kubernetes}, which is similar but more focused on scaling containers to an externally applied load, it is very lightweight.
It is also available in many Linux software repositories - such as for Debian - which makes the installation very easy.

Because there were no grave disadvantages found (depending on a third party library can always be considered be a disadvantage for flexibility, error-pronous and limit functionality) Nomad is being considered as a middle-ware to manage and deploy stages.
Others\cite{nomad:etc:gui_thesis} seem to be using Nomad to manage and deploy containers for similar reasons.
Nonetheless, further testing and prototyping will be required for a final decision.


\subsection{dCache}
\label{dcache}

\enquote{The goal of this project is to provide a system for storing and retrieving huge amounts of data, distributed among a large number of heterogenous server nodes, under a single virtual filesystem tree with a variety of standard access methods}\cite{dcache:main}.
dCache seems to be able to solve the storage access and distribution concern for the stages and sever nodes.
When using dCache, one could store the global resources distributed between the server nodes.
Built-in replication would prevent access loss on a node or network failure and an export through NFS\footnote{Network File System} allows easy access for Linux based systems\cite{dcache:overview:whitepaper}.

But the installation is complex and requires many services to be setup correctly, such as postgresql and many internal services such as zookeeper, admin, poolmanager, spacemanager, pnfsmanager, cleaner, gplazma, pinmanager, topo, info and nfs.
The documentation is also rather outdated and incomplete which meant, early tests with a prototype setup took days to setup and behaved rather unstable (probably due to a wrong configuration).
It is to be seen, whether such an complex and heavy system is actually required or if there are feasible alternatives.




\subsection{Further mentions}

The following list acknowledges programs that behave similar to the previously mentioned strategies or can be used as building blocks for custom implementations.
Programs that are listed here, were looked into, but not all in great depth because miss-fits or missing functionality were detected early on.
The list is in no specific order:

\begin{itemize}
	\item \textbf{Quartz}\cite{quartz:main} is a Java based program to schedule jobs. Instead of doing so by using input, Quartz executes programs through a timetable and in certain intervals.
	%	\cite{quartz:overview} 	\cite{quartz:quickstart}
	\item \textbf{Luigi}\cite{luigi:etc:distributed_pipelines} also executes pipelines with stages and is written in python. The advertised advantage is to define the pipeline directly in python code. But, this is at the same time the only way to define pipelines which contradictions with the existing Java TrackerApplication implementation.
	\item \textbf{Calery}\cite{celery:main} is focused on task execution through message passing and is written in Python. Intermediate results are expected to be transmitted through messages. Because there is no storage strategy and python adapter-code would have been required, Calery was dismissed.
	\item \textbf{IBM InfoSphere}\cite{infosphere:datastage} provides similar to Camunda a rich graphical user interface but for data transfer. Dismissed due to commercial nature.
	\item \textbf{qsub}\cite{qsub:etc:wiki}\cite{qsub:etc:uiowa} is a CLI\footnote{Command Line Interface} used in HPC to submit jobs onto a cluster or grid. Dismissed due to an expected high setup overhead, non-required multi-user nature and the fact, that it only provides a way to submit jobs.
	\item \textbf{CSCS}\cite{cscs:high_throughput} High Throughput Scheduler (GREASY). Dismissed for similar reasons as qsub, although it is more light weight and hardware agnostic (it can consider CUDA/GPU requirements).
	\item \textbf{zsync}\cite{zsync:main}, similar to rsync, is a file transfer program. Zsync allows to only transfer new parts when a file that shall be copied already exists in an older version on the target. This tool might be useful when implementing a custom resource distribution strategy is required.
	\item \textbf{OpenIO}\cite{openio:main}\label{openio} provides a distributed file system, is already provided as Docker image and provides a simple to use CLI. Because the NFS export is only available through a paid subscription plan, it was dismissed from further investigation.
	\item \textbf{SeaweedFS}\cite{seeweedfs:main}\label{seaweedfs} provides a scalable and distributed file system. The most interesting aspects are that it is rack-aware as well as natively supports external storage such as Amazon S3. When adding server nodes from the cloud this could allow all nodes to access the same file system while using rack-aware replication to reduce bandwidth usage and latency. A local test also proved that it is easy to setup, but because it cannot hot-swap nodes and was not able to recover when the seeweed master node became unreachable it was dismissed.
%	master halten die zuweisung file -> addresse
%	filer machen nur ein lookup und zuweisen oder sowas
%	aber der client frägt filer an
%	und der muss dann zu irgendeinem master die verbinundg aufbauen und nachschlagen
%	dh wollte pro physicalischen server 1 volume, 1 master, 1 filer haben
%	damit einfach dezentral kommen und gehen kann
%	aber problem 1: anzahl der master soll immer ungerade sein
%	problem 2: du kannst nicht einfach master on-the-fly hinzufügen und musst stattdessen teilweise die neu starten  mitm parameter: hey da drüben ist noch ein master
%	problem 3: es läuft nicht zuverlässig
	\item \textbf{Alluxio}\cite{alluxio:main}\label{alluxio} provides a distributed file system but was dismissed because it itself requires a centralized file system for the master and its fallback instances
	\item \textbf{GlusterFS}\cite{glusterfs:main}\label{glusterfs} is another tool to provide a distributed file system with replication. It was bought by IBM but is nonetheless available through the software repository of many Linux distributions such as Debian. A local test showed that the setup is very easy and no adjustments of configuration files are required. However, the replication mechanism requires that an integer multiple of nodes of the replica value are assigned to the file system. This makes GlusterFS hard to use in a scenario, where adding and removing nodes are expected to happen frequently. 
	% It was therefore dismissed.
%	\item \textbf{Ceph}\label{ceph}\todo{...}
\end{itemize}

\section{Docker}
\label{docker:image}

As describe in \autoref{workflow}, for easy deployment and setup Docker\cite{docker:main} containers shall be used.
This allows easier isolation of the executed stages and other host programs, as will be explain further on.

\begin{wrapfigure}{r}{0.30\textwidth}
	\centering
	\includegraphics[width=.2\textwidth]{res/docker-Moby-logo.png}
	\label{docker:logo}
	\caption{Official Docker \enquote{Moby} Logo\cite{docker:logo}}
	\vspace{1cm}
\end{wrapfigure}

Docker is the name of a software that combines various isolation technologies (\autoref{docker:technology}) to provide and execute third party software in virtual environments.
Docker aims are to increase security and to simplify installation as well as maintenance of applications.
Docker Swarm\cite{docker:swarm:key-concepts} and Kubernetes\cite{kubernetes:overview} use these fundamental features to scale applications, services and microservices in the cloud.
Software is retrieved from a public registry, called Docker Hub\cite{docker:hub} or from local or private registries.


\subsection{Technology}
\label{docker:technology}

Docker uses so called images to package and transport binaries with all their required libraries and configuration files as a read-only archive to the destination system.
Instead of spawning a new process for a binary in the host environment, the image is used to create a new virtual environment - the so called container.
Because the image includes all required libraries, it is possible to completely isolate the virtual environment which makes the host system invisible to any process inside the container.
Changes to files within containers are stored separately as differentials\footnote{for example by using OverlayFS\cite{overlayfs}}, which allows an image to be used by multiple containers at once.
Privileges, resource limitations and storage configurations can also be specified when creating a new container.
Processes inside containers are unable to see  other processes or files that are not part of or assigned to the their container\footnote{This is the default behaviour. It is possible to manually lift or modify many boundaries Docker enforces for containers on default.}.

In contrary to a hypervisor, docker is archiving this without hardware and operating system virtualization.
Instead of executing the container inside an additional virtualized operating system, they share the kernel of the host system.
For that, the host needs to support additional isolation mechanisms
At the time of writing this, only the Linux Kernel is capable to separate processes, network interfaces, interprocess communications, filesystem mounts and the time-sharing systems by namespaces.
By configuring these namespaces, Docker is capable to isolate containers into virtual environments.
Furthermore, control groups can be used to limit and constraint access to hardware resources.
\cite{docker:overview}

These approaches allow containers to run with very little additional overhead in comparison to running the application directly on the host.
Containerization increases security by limiting what an application sees and is able to interact with, decreases maintenance overhead because of no additional operation systems to maintain and allows to run multiple instances of the same application besides each other with independent configurations and environments but with the same base image.

\begin{comment}
\subsection{Self hosted registry}

\subsection{Deployment}

Dockerffile: creating a docker image

\subsection{Something something ref cloud }

docker is so popular that even microsoft is trying to support it, although most images require a Linux kernel - therefore microsoft introduced (WSL)

Paravirtualisation?

https://www.monkeyvault.net/docker-vs-virtualization/

Instead it uses built-in Linux Kernel containment features like CGroups, Namespaces, UnionFS, chroot (more on these later) to run applications in virtual environments. Those virtual environments - called Docker containers, have separate user lists, file systems or network devices.


\todo{layers? base image, less download, diffs}.
\end{comment}

\section{Data Storage}

When distributing workload onto different machines, accessing input and output files becomes another concern which is not present when all computation and storage resides on the same physical machine.
In theory, one could use portable mediums such as USB-Sticks, CD-ROMs or external HDDs, but in reality, this becomes very tedious really fast.
More advanced users might be able to take advantage of a common network connection between the computation to copy files and directories to the required places.
This strategy - still being tedious - surfaces another issue: dealing with multiple copies requires careful version management and additional storage space.
One certainly would not want to continue computation on outdated data or have too many copies of the same files.

The solution to that can be a network file system.
To programs this solution seems to be just another local directory hierarchy, but in reality, the files might be located on another or multiple other machines.

\begin{comment}
\subsection{Basics: file system?}

To understand how a network file system works, it is necessary to first understand the concept of a file system.
The following paragraph will focus on EXT(v4), which an open source file system used by many Linux servers, but the main concepts also apply to proprietary file systems such as NTFS from Microsoft.

The EXT (extended file system) family is based upon the concept of interlinked Inodes and \todo{data blocks}.
Data blocks describe large addressable binary objects that are linked by Inodes when allocated.
Inodes carry metadata to regulate access privileges and to represent a directory hierarchy.
To do so, the Inode has a type attribute that can declare it as a directory.
The data associated to the Inode is then interpreted as table of names and Inode references to entries of the directory the Inode represents.
In file mode, the Inode contains the content of the file.
For every Inode there is a certain amount of storage assigned inline allowing the Inode to store small files or directories in itself without referencing others and improves access times.

\todo{show (simplified) Inode+fields}

Furthermore, EXTv4 is a so called journaling file system.
\todo{interlink with DB ACID event system}


\todo{cite: link online man page ext4}

\subsection{POSIX Streams API}

\url{https://www.gnu.org/software/libc/manual/html_node/I_002fO-on-Streams.html}

The POSIX \todo{(Portable Operating System Interface)} defines a basic set of functions to work with files.
It is a standard that was standardized by IEE \todo{number} to define common functions across operating systems.
Simplified, and without caring about functions that create directories and links, move files or change attributes, the very basic functions to open and close streams are specified as the following C functions:

\begin{itemize}
\item \monospaceinline{FILE* fopen(const char *filename, const char *opentype)} to open a stream to the file which the path \monospaceinline{filename} refers to. The \monospaceinline{opentype} specifies whether the file shall be created, overwritten, opened in read-only, write-only or append mode.
\item \monospaceinline{int fclose(FILE *stream)} to close the given \monospaceinline{stream}
\item \monospaceinline{size_t fread(void *data, size_t size, size_t count, FILE *stream)} to read  \monospaceinline{count}-number of objects of  size \monospaceinline{size} from  \monospaceinline{stream} and to the \monospaceinline{data} memory location.
\item \monospaceinline{size_t fwrite(const void* data, size_t size, size_t count, FILE *stream)} to write \monospaceinline{count}-number of objects of size \monospaceinline{size} read from the \monospaceinline{data} memory location to the \monospaceinline{stream}
\item \monospaceinline{int fflush(FILE* stream)} to write buffered and not yet committed data to the underlying destination (filesystem).
\end{itemize}

The operating system usually delegate these calls to the corresponding file system driver for a given path.
The Linux kernel requires the filesystem to implement th following (simplified and incomplete) function listing:
\url{https://www.tldp.org/LDP/khg/HyperNews/get/fs/vfstour.html}

\begin{itemize}
\item \monospaceinline{int open(struct inode*, struct file*)} \todo{.}
\item \monospaceinline{int read(struct inode*, struct file*, char*, int)} \todo{.}
\item \monospaceinline{int write(struct inode*, struct file*, const char*, int)} \todo{.}
\item \monospaceinline{void release(struct inode*, struct file*)} \todo{.}
\item \monospaceinline{int fsync(struct inode*, struct file*)} \todo{.}
\end{itemize} 

A network file system needs to implement this interface and instead of writing and reading to and from a local device, it needs to send the requests to a remote storage server or to the storage cluster.

\end{comment}

\subsection{Remote File System}
\label{analysis:storage:organisation}

Remote file systems provide access to data that does not need to be stored on the local machine and can often be shared with other clients as well.
Some file systems are organized centralized, where at a single address and physical location all data is stored and retrieved from.
This can have advantages in setup time, maintenance and communication complexity, because of their simplicity.

The Network File Systems (NFS) for example, which is primarily used in the Linux and Unix world, is centralized, the clients are connecting to a remote server to read and write files from and to.
Since NFS 4.1 pNFS\footnote{parallel NFS}\cite[p. 14, section 1.7.2.2]{nfs:pnfs} allows the server to spread load across multiple servers and volumes, but because all metadata is still handled by one server, this approach still has a single point of failure.
The Common Internet File System (SMB/CIFS) is similar to NFS in regards of providing access to a remote file system locally, and it is the approach primarily used by Windows computers.
Similar to NFS, it has also a centralized approach.


More advanced file systems provide additional functionality like replication, which stores the data more than once to recover from data carrier failures, site awareness, which replicates data to geographically distant locations\footnote{to not be affected by a burning data centre for example or to decrease access time for clients from the distant location} and decentralized communication endpoints that provide access to the data from more than a single point of failure.


File systems like GlusterFS (\autoref{glusterfs}), Alluxio (\autoref{alluxio}), SeaweedFS (\autoref{seaweedfs}), OpenIO (\autoref{openio}), dCache (\autoref{dcache}) and HadoopFS (\autoref{hadoopfs}) try to provide these functionality.

\section{Angular Web-Application and REST API}
\label{fundamental:angular}

The user interface is supposed to be implemented by an Angular Web-Application.
Angular\cite{angular} itself is a framework for mobile and desktop Web-Applications which uses templating to generate the website completely on the client.
It uses TypeScript\cite{typescript}, which is, in essence, an enriched subset of JavaScript with type annotations, that compiles back to JavaScript.
This allows any recent web-browser to execute TypeScript \enquote{code}, without the need to explicitly supporting it.
The additional type annotation in TypeScript supports developers by giving helpful error messages, instead of runtime errors.

This thesis uses REST REST (Representation State Transfer) to enable the Angular application to communicate with the web-server.
REST is, by now, commonly used to transfer state on top of HTTP in the internet.
It utilizes the HTTP methods\cite{http:methods} to request resources or to push updates using URLs for localization.

\todo{needs more work -- and detail?}

\section{Agile development}
\label{fundamental:agile}

\todo{some notes regarding user stories, backlog, prioritization and sprints?}