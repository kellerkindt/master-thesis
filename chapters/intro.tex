\chapter{Introduction}

Since the industrial revolution, humans strive for more automation in the industry as well as in the every day life.
What was at first a cost saving measurement in factories, now also is a differentiation method for products.
A new product must prove a higher level comfort to the customer than the previous generation as well as all the competitors.
As such, the ambitions of the industry are focused on increasing the value of their products for the customer.

The automotive industry is one of the prime examples of this.
Never was traveling from one place to another as comfortable as nowadays.
Aspects like an elegant interior design, comfortable seats, air conditioning, entertainment systems and safety measurements need to be considered by car manufacturers to be competitive these days. 
The next step \todo{onward in this battle} for the most luxurious driving experience is the autonomous driving vehicle.
No longer shall the owner of a car steer it, but instead the car becomes his or hers personal chauffeur, driving the optimal route, the most comfortable way and being more reliable and safer than any human ever could.

The reason, autonomously driving cars are not common yet, is the complexity of it.
Compared to existing technologies like parking assistants, entertainment systems or simply more efficient engine controllers, letting a computer reliably understand a certain traffic situation requires masses input data.
As such, the problem itself becomes massive.
The industry has no choice than to divide it into many small pieces to be able to conquer solutions to it (\todo{ref divide and conquer?}).

The MEC-View research project is such a puzzle piece.
It explorers whether and how to include external, steady mounted sensors in the decision \todo{finding} of partly autonomous vehicles in situations, where onboard sensors are \todo{limited}.
The decision finding process also requires one to understand traffic flows
What might seem to be the right decision for a single car, might be disastrous for the traffic flow, such as being the cause for a \todo{Phantomstau}.
To understand traffic flows, one needs to study and thereby watch real traffic.

This thesis will conceptualize and realize a distributed and automated computer vision pipeline which analyzes traffic flow within video footage.
Such footage is captured for the above described scenario within the MEC-View research project.



\section{MEC-View?}

\section{The Tool}

describe the tool, what it is for, what it does, current workflow

The current workflow consists of the following steps:

- define reference points in one single frame through a user input

- track stationary reference points on all other frames

- estimate the camera position for each frame

- detect vehicles in all frames

- track detections and assign them to trajectories

- perform lane detection

- record a result video

- export trajectories to a csv-file

- create charts



As listed above, at least one stage must be able to process user-input. The

current progress must be observed and errors must be reported in an way, that

allows one to understand the circumstances for the cause of the error.

For easy and fast scalability, docker images shall be used to distribute the

binaries onto the nodes.


\section{Defining the Problem Space}

what is required / what shall the implementation be capable of from the view of the "user"

user interaction



\section{Analyzing the Problem Space}

describe scenarios the implementations must be able to handle in order to archive the requirements?

resource tracking
 - global (read-only) input resources ("big" data files)
 - per stage evolving project files
   - might have some kind of version control?
 - dynamically detect within a stage whether user input is required
 - be able to continue / redo latest stage
 - error / warning detection / tracking!
   ( [!a-zA-z]err[!a-zA-z])|( [!a-zA-z]error[!a-zA-z])
 - web technology

- retrieve required binaries
- retrieve required resource files
- archive output files and logs

- persistence stage/state tracking of projects/pipelines/states

Problems to solve



- stages might have individual hardware requirements

- multiple stages might require the same hardware at the same time

- stages can depend on the result of another stage

- for scalability, it shall be easy to add and remove hardware-nodes

- the video files are large (4k footage), sending decoded frames (~25MB)

through the network might be unreasonable

- the definition of a pipeline shall be easy to understand for good

maintainability

- the hardware shall be used efficiently to achieve a high throughput

- docker images need to contain and provide all required libraries

- prevent stages from leaving other stages far behind?

- storage and distribution of intermediate results

- log collection




adding a new host
 - instantiate docker image and mount config and docker socket?
 - encrypt communication between control and worker?
 - possibility for decentralization
    - makes archiving logs and results hard


scenarios

define pipeline
 - define gpu stage
 - define cpu stage
 - define required input assets
 - define assets for each stage to be accessible in the next stage
 - stages depend on other stages
    - do it the other way around? set next stage?
       - next stage + "parameters" (assets to keep/transfer)
       - allows branching

upload resource file (video)
 - ... upload <path> <name-at-remote>
 - maybe to one common pool of resources?
    - free disk space?

start pipeline
 - select resources required by the pipeline
 - start
 
go through stages until finished
 - take care of cpu/gpu env requirements
 - if no common pool of resources: concurrently copy assets to target machines
 - archive 

maybe: halt at stage because of error / required user interaction
 - allow continuation
 - allow download / upload of assets into this stage
    - free disk space?
 
 
 easy installation and binary distribution
  - docker image per pipeline stage?
  - map management binary into docker -> exec
     - requires standalone binary
     - implicitly requires compatible libc env/unix system
     - requires administrative (docker ) privileges
     
 
 outputs of a stage are immutable after it has finished, stages using that data are working on a copy
 
 nice to have: display progress captured from log (regex filter with multiple subjects/progresses per stage)
 
 show time a stage is running
 
 show estimated remaining time (based on captured progress)
     
  todo list per project
  
  project can run through multiple pipelines multiple times
  
  nomad -> .deb archive?
  
deployment
 - web
 - controller
 - third party / nomad
     
start
start from a certain stage
pause after a stage
redo a stage
change variables at a stage