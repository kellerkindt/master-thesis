\chapter{Fundamentals}

This chapter will explain and discuss fundamental knowledge required for the then following chapters.

\section{Docker}

\begin{wrapfigure}{r}{0.45\textwidth}
	\centering
	\includegraphics[width=.2\textwidth]{res/docker-Moby-logo.png}
	\label{docker:logo}
	\caption{Official Docker \enquote{Moby} Logo\cite{docker:logo}}
\end{wrapfigure}

Docker is the name of a software that combines isolation technologies (\autoref{docker:technology}) and a rich ecosystem (\autoref{docker:ecosystem}) to provide and execute third party software in virtual environments.
Docker aims to increase security and to simplify installation and maintenance of applications.


\subsection{Technology}
\label{docker:technology}

Docker uses so called images to package and transport binaries with all their required libraries and configuration files as a read-only archive to the destination host.
\todo{layers? base image, less download, diffs}
To spawn a new process for a binary within an image, a new virtual environment - the so called container - needs to be setup first.
Changes to files within containers are stored in separate differentials, which allows the image to be used by multiple containers at once.
Privileges, resource limitations and storage configurations are also part of a container definition.
Processes inside containers are unable to see  other processes or files that are not part of or assigned to the their container\footnote{This is the default behavior. It is possible to manually lift or modify many boundaries Docker enforces for containers on default.}.

In contrary to a hypervisor, docker is archiving this without running additional virtual machines for each container.
Instead of running on virtualized operating systems, container processes share the host kernel.
In order to do so, the host operating system needs to support additional isolation mechanisms.
At the time of writing this, only the Linux Kernel is capable to separate processes, network interfaces, interprocess communications, filesystem mounts and the timesharing systems by namespaces.
By configuring these namespaces, Docker is capable to isolate containers into virtual environments.
Furthermore, control groups can be used to limit and constraint access to hardware resources.
\cite{docker:overview}

These approaches allow containers to run with very little overhead in comparison to running the application directly on the host.
Containerization increases security by limiting what an application sees and is able to interact with, decreases maintenance overhead because of no additional operation systems to maintain and allows to run multiple instances of the same application besides each other with independent configurations and environments.

\subsection{Architecture and Ecosystem}
\label{docker:ecosystem}

\subsection{Self hosted registry}

\subsection{Deployment}

Dockerffile: creating a docker image

\subsection{Something something ref cloud }

docker is so popular that even microsoft is trying to support it, although most images require a Linux kernel - therefore microsoft introduced (WSL)

Paravirtualisation?

https://www.monkeyvault.net/docker-vs-virtualization/

Instead it uses built-in Linux Kernel containment features like CGroups, Namespaces, UnionFS, chroot (more on these later) to run applications in virtual environments. Those virtual environments - called Docker containers, have separate user lists, file systems or network devices.

\section{Network File System}

When distributing workload onto different machines, accessing input and output files becomes another concern which is not present when computation and storage reside on the same physical machine.
In theory, one could use portable mediums such as USB-Sticks, CD-ROMs or external HDDs, but in reality, this becomes to tidies very fast.
More advanced users might be able to take advantage of a common network connection between the computation and storage nodes to copy files and directories from and to the required places.
This strategy - still being tidies - surfaces another issue: having multiple copies requires careful version management and additional storage space.
One certainly would not want to continue computation on outdated files or have too many copies of the same files when the required next file no longer fits on the computation node.

The solution is to use what is called network file system.
To programs they seem to be just another local directory hierarchy, but in reality, the files might be located on another or multiple other machines.

\subsection{Basics: file system?}

To understand how a network file system works, it is necessary to first understand the concept of a file system.
The following paragraph will focus on EXT(v4), which an open source file system used by many Linux servers, but the main concepts also apply to proprietary file systems such as NTFS from Microsoft.

The EXT (extended file system) family is based upon the concept of interlinked Inodes and \todo{data blocks}.
Data blocks describe large addressable binary objects that are linked by Inodes when allocated.
Inodes carry metadata to regulate access privileges and to represent a directory hierarchy.
To do so, the Inode has a type attribute that can declare it as a directory.
The data associated to the Inode is then interpreted as table of names and Inode references to entries of the directory the Inode represents.
In file mode, the Inode contains the content of the file.
For every Inode there is a certain amount of storage assigned inline allowing the Inode to store small files or directories in itself without referencing others and improves access times.

\todo{show (simplified) Inode+fields}

Furthermore, EXTv4 is a so called journaling file system.
\todo{interlink with DB ACID event system}


\todo{cite: link online man page ext4}

\subsection{POSIX Streams API}

\url{https://www.gnu.org/software/libc/manual/html_node/I_002fO-on-Streams.html}

The POSIX \todo{(Portable Operating System Interface)} defines a basic set of functions to work with files.
It is a standard that was standardized by IEE \todo{number} to define common functions across operating systems.
Simplified, and without caring about functions that create directories and links, move files or change attributes, the very basic functions to open and close streams are specified as the following C functions:

\begin{itemize}
	\item \monospaceinline{FILE* fopen(const char *filename, const char *opentype)} to open a stream to the file which the path \monospaceinline{filename} refers to. The \monospaceinline{opentype} specifies whether the file shall be created, overwritten, opened in read-only, write-only or append mode.
	\item \monospaceinline{int fclose(FILE *stream)} to close the given \monospaceinline{stream}
	\item \monospaceinline{size_t fread(void *data, size_t size, size_t count, FILE *stream)} to read  \monospaceinline{count}-number of objects of  size \monospaceinline{size} from  \monospaceinline{stream} and to the \monospaceinline{data} memory location.
	\item \monospaceinline{size_t fwrite(const void* data, size_t size, size_t count, FILE *stream)} to write \monospaceinline{count}-number of objects of size \monospaceinline{size} read from the \monospaceinline{data} memory location to the \monospaceinline{stream}
	\item \monospaceinline{int fflush(FILE* stream)} to write buffered and not yet committed data to the underlying destination (filesystem).
\end{itemize}

The operating system usually delegate these calls to the corresponding file system driver for a given path.
The Linux kernel requires the filesystem to implement th following (simplified and incomplete) function listing:
\url{https://www.tldp.org/LDP/khg/HyperNews/get/fs/vfstour.html}

\begin{itemize}
	\item \monospaceinline{int open(struct inode*, struct file*)} \todo{.}
	\item \monospaceinline{int read(struct inode*, struct file*, char*, int)} \todo{.}
	\item \monospaceinline{int write(struct inode*, struct file*, const char*, int)} \todo{.}
	\item \monospaceinline{void release(struct inode*, struct file*)} \todo{.}
	\item \monospaceinline{int fsync(struct inode*, struct file*)} \todo{.}
\end{itemize} 

A network file system needs to implement this interface and instead of writing and reading to and from a local device, it needs to send the requests to a remote storage server or to the storage cluster.


\subsection{Centralized Network Storage}

NFS, SMB

\subsection{Decentralized}

ceph, glusterfs, hadoop



\section{HTTP and REST}
